{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"source":["# Sentiment Analysis of IMDB Movie Reviews (Part 2: Machine Learning models)"]},{"cell_type":"markdown","metadata":{},"source":["**Problem Statement:**\n","\n","In this, we have to predict the number of positive and negative reviews based on sentiments by using different classification models."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Side note for profs\n","- I have done NLP analysis before,so this work is using as a [this template](https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews/notebook) as a baseline. \n","- Feel free to ignore the BERT model code. It works but taking too long to train and often crashes the kernal. Hence, I switch to use HuggingFace training package in Sentiment Analysis of IMDB Movie Reviews (Part 3: BERT model)"]},{"cell_type":"markdown","metadata":{"_uuid":"1424638f5259100af9f9a5c1b05bd23cf5b71e51"},"source":["## Data Processing \n","**Import necessary libraries**"]},{"cell_type":"code","execution_count":145,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2023-03-01T09:40:47.288416Z","iopub.status.busy":"2023-03-01T09:40:47.287988Z","iopub.status.idle":"2023-03-01T09:40:51.815177Z","shell.execute_reply":"2023-03-01T09:40:51.814112Z","shell.execute_reply.started":"2023-03-01T09:40:47.288352Z"},"trusted":true},"outputs":[],"source":["#Load the libraries\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import nltk\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelBinarizer\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from wordcloud import WordCloud,STOPWORDS\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize,sent_tokenize\n","from bs4 import BeautifulSoup\n","import spacy\n","import re,string,unicodedata\n","from nltk.tokenize.toktok import ToktokTokenizer\n","from nltk.stem import LancasterStemmer,WordNetLemmatizer\n","from sklearn.linear_model import LogisticRegression,SGDClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","# from textblob import TextBlob\n","# from textblob import Word\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"]},{"cell_type":"markdown","metadata":{"_uuid":"be1b642cce343f7a8f68f8c91f7c50372cdf4381"},"source":["**Import the training dataset**"]},{"cell_type":"code","execution_count":146,"metadata":{"_uuid":"4c593c17588723c0b0b0f19851cb70a8447ced76","execution":{"iopub.execute_input":"2023-03-01T09:40:51.820197Z","iopub.status.busy":"2023-03-01T09:40:51.819814Z","iopub.status.idle":"2023-03-01T09:40:53.513695Z","shell.execute_reply":"2023-03-01T09:40:53.512848Z","shell.execute_reply.started":"2023-03-01T09:40:51.820111Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(50000, 2)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Probably my all-time favorite movie, a story o...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>I sure would like to see a resurrection of a u...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>This show was an amazing, fresh &amp; innovative i...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Encouraged by the positive comments about this...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>If you like original gut wrenching laughter yo...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n","5  Probably my all-time favorite movie, a story o...  positive\n","6  I sure would like to see a resurrection of a u...  positive\n","7  This show was an amazing, fresh & innovative i...  negative\n","8  Encouraged by the positive comments about this...  negative\n","9  If you like original gut wrenching laughter yo...  positive"]},"execution_count":146,"metadata":{},"output_type":"execute_result"}],"source":["#importing the training data\n","imdb_data=pd.read_csv('IMDB Dataset.csv')\n","print(imdb_data.shape)\n","imdb_data.head(10)"]},{"cell_type":"markdown","metadata":{"_uuid":"1ad3773974351ed9bdf389b2847d7475b36c2295"},"source":["**Exploratery data analysis**"]},{"cell_type":"code","execution_count":147,"metadata":{"_uuid":"7f11c83b1320c8982b36889145f7f770563674a8","execution":{"iopub.execute_input":"2023-03-01T09:40:53.515634Z","iopub.status.busy":"2023-03-01T09:40:53.515078Z","iopub.status.idle":"2023-03-01T09:40:53.658086Z","shell.execute_reply":"2023-03-01T09:40:53.657026Z","shell.execute_reply.started":"2023-03-01T09:40:53.515585Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>50000</td>\n","      <td>50000</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>49582</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>Loved today's show!!! It was a variety and not...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>5</td>\n","      <td>25000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   review sentiment\n","count                                               50000     50000\n","unique                                              49582         2\n","top     Loved today's show!!! It was a variety and not...  positive\n","freq                                                    5     25000"]},"execution_count":147,"metadata":{},"output_type":"execute_result"}],"source":["#Summary of the dataset\n","imdb_data.describe()"]},{"cell_type":"markdown","metadata":{"_uuid":"453c3fd238f62ab8f649eb01771817e25bc0c77d"},"source":["**Sentiment count**"]},{"cell_type":"code","execution_count":148,"metadata":{"_uuid":"cb6bb97b0f851947dcf341a1de5708a1f2bc64c1","execution":{"iopub.execute_input":"2023-03-01T09:40:53.659995Z","iopub.status.busy":"2023-03-01T09:40:53.659583Z","iopub.status.idle":"2023-03-01T09:40:53.674596Z","shell.execute_reply":"2023-03-01T09:40:53.673357Z","shell.execute_reply.started":"2023-03-01T09:40:53.659924Z"},"trusted":true},"outputs":[{"data":{"text/plain":["positive    25000\n","negative    25000\n","Name: sentiment, dtype: int64"]},"execution_count":148,"metadata":{},"output_type":"execute_result"}],"source":["#sentiment count\n","imdb_data['sentiment'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["We can see that the dataset is balanced."]},{"cell_type":"markdown","metadata":{"_uuid":"f61964573faababe1f7897b77d32815a24954d2f"},"source":["**Spliting the training dataset**"]},{"cell_type":"code","execution_count":149,"metadata":{"_uuid":"d3aaabff555e07feb11c72cc3a6e457615975ffe","execution":{"iopub.execute_input":"2023-03-01T09:40:56.888054Z","iopub.status.busy":"2023-03-01T09:40:56.887014Z","iopub.status.idle":"2023-03-01T09:40:56.903236Z","shell.execute_reply":"2023-03-01T09:40:56.902004Z","shell.execute_reply.started":"2023-03-01T09:40:56.887933Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(40000,) (40000,)\n","(10000,) (10000,)\n"]}],"source":["#split the dataset  \n","#train dataset\n","train_reviews=imdb_data.review[:40000]\n","train_sentiments=imdb_data.sentiment[:40000]\n","#test dataset\n","test_reviews=imdb_data.review[40000:]\n","test_sentiments=imdb_data.sentiment[40000:]\n","print(train_reviews.shape,train_sentiments.shape)\n","print(test_reviews.shape,test_sentiments.shape)"]},{"cell_type":"markdown","metadata":{"_uuid":"328b6e5977da3e055ad4b2e11a31e5e12ccf3b16"},"source":["**Removing html strips and noise text**"]},{"cell_type":"code","execution_count":150,"metadata":{"_uuid":"6f6fcafbdadcdcb0c164e37d71fb9d1623f74d0a","execution":{"iopub.execute_input":"2023-03-01T09:40:56.905765Z","iopub.status.busy":"2023-03-01T09:40:56.905088Z","iopub.status.idle":"2023-03-01T09:41:07.044842Z","shell.execute_reply":"2023-03-01T09:41:07.043888Z","shell.execute_reply.started":"2023-03-01T09:40:56.905459Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/swimmingcircle/opt/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  warnings.warn(\n"]}],"source":["#Removing the html strips\n","def strip_html(text):\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    return soup.get_text()\n","\n","#Removing the square brackets\n","def remove_between_square_brackets(text):\n","    return re.sub('\\[[^]]*\\]', '', text)\n","\n","#Removing the noisy text\n","def denoise_text(text):\n","    text = strip_html(text)\n","    text = remove_between_square_brackets(text)\n","    return text\n","#Apply function on review column\n","imdb_data['review']=imdb_data['review'].apply(denoise_text)"]},{"cell_type":"markdown","metadata":{"_uuid":"88117b74761d1047924d6d70f76642faa0e706ac"},"source":["**Removing special characters**"]},{"cell_type":"code","execution_count":151,"metadata":{"_uuid":"219da72b025121fd98081df50ae0fcaace10cc9d","execution":{"iopub.execute_input":"2023-03-01T09:41:07.046463Z","iopub.status.busy":"2023-03-01T09:41:07.046110Z","iopub.status.idle":"2023-03-01T09:41:08.611027Z","shell.execute_reply":"2023-03-01T09:41:08.609637Z","shell.execute_reply.started":"2023-03-01T09:41:07.046400Z"},"trusted":true},"outputs":[],"source":["#Define function for removing special characters\n","def remove_special_characters(text, remove_digits=True):\n","    pattern=r'[^a-zA-z0-9\\s]'\n","    text=re.sub(pattern,'',text)\n","    return text\n","#Apply function on review column\n","imdb_data['review']=imdb_data['review'].apply(remove_special_characters)"]},{"cell_type":"markdown","metadata":{"_uuid":"3b66eeabd5b7b8c251f8b8ddf331140a64bcd514"},"source":["**Text stemming**"]},{"cell_type":"code","execution_count":152,"metadata":{"_uuid":"2295f2946e0ab74c220ad538d0e7adc04d23f697","execution":{"iopub.execute_input":"2023-03-01T09:41:08.612931Z","iopub.status.busy":"2023-03-01T09:41:08.612606Z","iopub.status.idle":"2023-03-01T09:45:47.976240Z","shell.execute_reply":"2023-03-01T09:45:47.974827Z","shell.execute_reply.started":"2023-03-01T09:41:08.612871Z"},"trusted":true},"outputs":[],"source":["#Stemming the text\n","def simple_stemmer(text):\n","    ps=nltk.porter.PorterStemmer()\n","    text= ' '.join([ps.stem(word) for word in text.split()])\n","    return text\n","#Apply function on review column\n","imdb_data['review']=imdb_data['review'].apply(simple_stemmer)"]},{"cell_type":"markdown","metadata":{"_uuid":"e83107e4a281d84d7ae42b4e2c8d81b7ece438e4"},"source":["**Removing stopwords**"]},{"cell_type":"code","execution_count":153,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T09:45:47.978597Z","iopub.status.busy":"2023-03-01T09:45:47.978169Z","iopub.status.idle":"2023-03-01T09:45:47.992670Z","shell.execute_reply":"2023-03-01T09:45:47.991402Z","shell.execute_reply.started":"2023-03-01T09:45:47.978524Z"},"trusted":true},"outputs":[],"source":["#Tokenization of text\n","tokenizer=ToktokTokenizer()\n","#Setting English stopwords\n","stopword_list=nltk.corpus.stopwords.words('english')"]},{"cell_type":"code","execution_count":154,"metadata":{"_uuid":"5dbff82b4d2d188d8777b273a75d8ac714d38885","execution":{"iopub.execute_input":"2023-03-01T09:45:47.994664Z","iopub.status.busy":"2023-03-01T09:45:47.994320Z","iopub.status.idle":"2023-03-01T09:46:33.731867Z","shell.execute_reply":"2023-03-01T09:46:33.730660Z","shell.execute_reply.started":"2023-03-01T09:45:47.994616Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'until', 'all', 'didn', 'when', 'before', \"mightn't\", \"you're\", 'herself', 'that', \"you'd\", 'doing', 't', 'while', 'needn', 'ma', \"isn't\", 'their', 'has', 'him', 'mustn', 'an', 'd', 'in', 'same', 'then', 'being', 'both', 'itself', 'very', \"shouldn't\", 'hasn', \"wouldn't\", 'having', 'some', \"weren't\", 'ourselves', 'ours', 'against', 's', 'through', \"haven't\", \"wasn't\", \"doesn't\", 'ain', 'been', \"she's\", 'a', 'why', 'because', 'down', 'further', 'hadn', 'wasn', 'theirs', 'its', 'themselves', 'were', 'between', \"you've\", 'from', 'doesn', 've', 'should', 'so', \"mustn't\", 'himself', 'nor', 'more', 'couldn', 'had', \"didn't\", 'can', 'them', 'shouldn', 'for', 'o', 'don', 'which', 'to', 'shan', 'again', \"it's\", 'isn', 'of', 'yourselves', \"shan't\", 'no', 'yours', 'under', 'you', 'up', 'are', 'i', 'on', 'aren', 'and', 'about', 'y', 'll', \"needn't\", 'once', 'will', 'at', 'her', 'by', 'wouldn', 're', \"won't\", 'other', 'there', 'what', 'she', 'or', 'haven', \"hasn't\", 'the', 'me', 'if', 'mightn', 'they', 'but', 'now', \"hadn't\", 'who', 'won', 'off', 'each', 'too', 'he', 'his', \"don't\", 'it', 'how', 'does', 'is', 'few', 'be', 'below', 'not', 'into', 'am', 'as', 'over', \"should've\", 'most', 'hers', 'm', 'weren', 'after', 'do', 'our', 'only', 'these', 'those', 'own', 'have', 'did', 'any', \"couldn't\", \"that'll\", 'just', 'than', 'myself', 'yourself', 'whom', 'your', 'my', 'with', \"you'll\", 'we', 'this', 'where', 'such', \"aren't\", 'above', 'was', 'here', 'out', 'during'}\n"]}],"source":["#set stopwords to english\n","stop=set(stopwords.words('english'))\n","print(stop)\n","\n","#removing the stopwords\n","def remove_stopwords(text, is_lower_case=False):\n","    tokens = tokenizer.tokenize(text)\n","    tokens = [token.strip() for token in tokens]\n","    if is_lower_case:\n","        filtered_tokens = [token for token in tokens if token not in stopword_list]\n","    else:\n","        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n","    filtered_text = ' '.join(filtered_tokens)    \n","    return filtered_text\n","#Apply function on review column\n","imdb_data['review']=imdb_data['review'].apply(remove_stopwords)"]},{"cell_type":"markdown","metadata":{"_uuid":"b35e7499291173119ed42287deac6f0cd96516e1"},"source":["**Normalized train reviews**"]},{"cell_type":"code","execution_count":155,"metadata":{"_kg_hide-output":true,"_uuid":"b20c242bd091929ca896ea2c6e936ca00efe6ecf","execution":{"iopub.execute_input":"2023-03-01T09:46:33.734477Z","iopub.status.busy":"2023-03-01T09:46:33.734022Z","iopub.status.idle":"2023-03-01T09:46:33.744487Z","shell.execute_reply":"2023-03-01T09:46:33.743410Z","shell.execute_reply.started":"2023-03-01T09:46:33.734384Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'one review ha mention watch 1 oz episod youll hook right thi exactli happen meth first thing struck oz wa brutal unflinch scene violenc set right word go trust thi show faint heart timid thi show pull punch regard drug sex violenc hardcor classic use wordit call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti wa surreal couldnt say wa readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort viewingthat get touch darker side'"]},"execution_count":155,"metadata":{},"output_type":"execute_result"}],"source":["#normalized train reviews\n","norm_train_reviews=imdb_data.review[:40000]\n","norm_train_reviews[0]"]},{"cell_type":"markdown","metadata":{"_uuid":"d69462bb209a66cff86376dc8481d0c0140d894d"},"source":["**Normalized test reviews**"]},{"cell_type":"code","execution_count":156,"metadata":{"_kg_hide-output":true,"_uuid":"c5d0d38bd9976150367e9d75f3b933774c96a1ab","execution":{"iopub.execute_input":"2023-03-01T09:46:33.746902Z","iopub.status.busy":"2023-03-01T09:46:33.746496Z","iopub.status.idle":"2023-03-01T09:46:33.759122Z","shell.execute_reply":"2023-03-01T09:46:33.757724Z","shell.execute_reply.started":"2023-03-01T09:46:33.746828Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'read review watch thi piec cinemat garbag took least 2 page find somebodi els didnt think thi appallingli unfunni montag wasnt acm humour 70 inde ani era thi isnt least funni set sketch comedi ive ever seen itll till come along half skit alreadi done infinit better act monti python woodi allen wa say nice piec anim last 90 second highlight thi film would still get close sum mindless drivelridden thi wast 75 minut semin comedi onli world semin realli doe mean semen scatolog humour onli world scat actual fece precursor joke onli mean thi handbook comedi tit bum odd beaver niceif pubesc boy least one hand free havent found playboy exist give break becaus wa earli 70 way sketch comedi go back least ten year prior onli way could even forgiv thi film even made wa gunpoint retro hardli sketch clown subtli pervert children may cut edg circl could actual funni come realli quit sad kept go throughout entir 75 minut sheer belief may save genuin funni skit end gave film 1 becaus wa lower scoreand onli recommend insomniac coma patientsor perhap peopl suffer lockjawtheir jaw would final drop open disbelief'"]},"execution_count":156,"metadata":{},"output_type":"execute_result"}],"source":["#Normalized test reviews\n","norm_test_reviews=imdb_data.review[40000:]\n","norm_test_reviews[45005]"]},{"cell_type":"markdown","metadata":{"_uuid":"1c2a872ffcb6b8076fdbbba641af12081b6022ef"},"source":["**Bags of words model**\n","\n","It is used to convert text documents to numerical vectors or bag of words."]},{"cell_type":"code","execution_count":157,"metadata":{"_uuid":"35cf9dcefb40b2dc520c5b0d559695324c46cc04","execution":{"iopub.execute_input":"2023-03-01T09:46:33.760681Z","iopub.status.busy":"2023-03-01T09:46:33.760424Z","iopub.status.idle":"2023-03-01T09:47:48.952148Z","shell.execute_reply":"2023-03-01T09:47:48.951099Z","shell.execute_reply.started":"2023-03-01T09:46:33.760644Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BOW_cv_train: (40000, 6209089)\n","BOW_cv_test: (10000, 6209089)\n"]}],"source":["#Count vectorizer for bag of words\n","cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n","#transformed train reviews\n","cv_train_reviews=cv.fit_transform(norm_train_reviews)\n","#transformed test reviews\n","cv_test_reviews=cv.transform(norm_test_reviews)\n","\n","print('BOW_cv_train:',cv_train_reviews.shape)\n","print('BOW_cv_test:',cv_test_reviews.shape)\n","#vocab=cv.get_feature_names()-toget feature names"]},{"cell_type":"markdown","metadata":{"_uuid":"52371868f05ff9cf157280c5acf0f5bc71ee176d"},"source":["**Term Frequency-Inverse Document Frequency model (TFIDF)**\n","\n","It is used to convert text documents to  matrix of  tfidf features."]},{"cell_type":"code","execution_count":158,"metadata":{"_uuid":"afe6de957339921e05a6faeaf731f2272fd31946","execution":{"iopub.execute_input":"2023-03-01T09:47:48.954080Z","iopub.status.busy":"2023-03-01T09:47:48.953766Z","iopub.status.idle":"2023-03-01T09:49:08.309825Z","shell.execute_reply":"2023-03-01T09:49:08.308470Z","shell.execute_reply.started":"2023-03-01T09:47:48.954012Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tfidf_train: (40000, 6209089)\n","Tfidf_test: (10000, 6209089)\n"]}],"source":["#Tfidf vectorizer\n","tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n","#transformed train reviews\n","tv_train_reviews=tv.fit_transform(norm_train_reviews)\n","#transformed test reviews\n","tv_test_reviews=tv.transform(norm_test_reviews)\n","print('Tfidf_train:',tv_train_reviews.shape)\n","print('Tfidf_test:',tv_test_reviews.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Logistic Regression"]},{"cell_type":"code","execution_count":159,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["LogisticRegression(C=1, max_iter=500, random_state=42)\n","LogisticRegression(C=1, max_iter=500, random_state=42)\n"]}],"source":["#training the model\n","lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n","#Fitting the model for Bag of words\n","lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n","print(lr_bow)\n","#Fitting the model for tfidf features\n","lr_tfidf=lr.fit(tv_train_reviews,train_sentiments)\n","print(lr_tfidf)"]},{"cell_type":"code","execution_count":160,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['negative' 'negative' 'negative' ... 'negative' 'positive' 'positive']\n","['negative' 'negative' 'negative' ... 'negative' 'positive' 'positive']\n"]}],"source":["#Predicting the model for bag of words\n","lr_bow_predict=lr.predict(cv_test_reviews)\n","print(lr_bow_predict)\n","##Predicting the model for tfidf features\n","lr_tfidf_predict=lr.predict(tv_test_reviews)\n","print(lr_tfidf_predict)"]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["lr_bow_score : 0.7512\n","lr_tfidf_score : 0.75\n"]}],"source":["#Accuracy score for bag of words\n","lr_bow_score=accuracy_score(test_sentiments,lr_bow_predict)\n","print(\"lr_bow_score :\",lr_bow_score)\n","#Accuracy score for tfidf features\n","lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n","print(\"lr_tfidf_score :\",lr_tfidf_score)"]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","    Positive       0.75      0.75      0.75      4993\n","    Negative       0.75      0.75      0.75      5007\n","\n","    accuracy                           0.75     10000\n","   macro avg       0.75      0.75      0.75     10000\n","weighted avg       0.75      0.75      0.75     10000\n","\n","              precision    recall  f1-score   support\n","\n","    Positive       0.74      0.77      0.75      4993\n","    Negative       0.76      0.73      0.75      5007\n","\n","    accuracy                           0.75     10000\n","   macro avg       0.75      0.75      0.75     10000\n","weighted avg       0.75      0.75      0.75     10000\n","\n"]}],"source":["#Classification report for bag of words \n","lr_bow_report=classification_report(test_sentiments,lr_bow_predict,target_names=['Positive','Negative'])\n","print(lr_bow_report)\n","\n","#Classification report for tfidf features\n","lr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['Positive','Negative'])\n","print(lr_tfidf_report)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Logistic analysis shows around 75% accuracy for both bags of words and tf-idf data which is quite impressive!"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## SDGC: Stochastic gradient descent or Linear support vector machines for bag of words and tfidf features\n","\n","SGDClassifier is a linear classifier in scikit-learn that uses stochastic gradient descent (SGD) as the optimization algorithm. It is a type of online learning algorithm that can handle large-scale datasets efficiently, by processing one instance at a time and updating the model parameters incrementally.\n","\n","SGDClassifier can be used for binary classification, multi-class classification, and regression tasks. It supports a variety of loss functions, such as hinge loss (for linear SVM), log loss (for logistic regression), and squared loss (for linear regression). It also supports various regularization methods, such as L1 and L2 regularization, to prevent overfitting.\n","\n","SGDClassifier can be a good choice for large datasets or streaming data, where batch learning algorithms may not be suitable due to memory constraints or processing time. However, it may require more hyperparameter tuning and preprocessing than other classifiers, as it is more sensitive to the scaling and distribution of the features."]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["SGDClassifier(max_iter=500, random_state=42)\n","SGDClassifier(max_iter=500, random_state=42)\n"]}],"source":["#training the linear svm\n","svm=SGDClassifier(loss='hinge',max_iter=500,random_state=42)\n","#fitting the svm for bag of words\n","svm_bow=svm.fit(cv_train_reviews,train_sentiments)\n","print(svm_bow)\n","#fitting the svm for tfidf features\n","svm_tfidf=svm.fit(tv_train_reviews,train_sentiments)\n","print(svm_tfidf)"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['positive' 'positive' 'negative' ... 'positive' 'positive' 'positive']\n","['positive' 'positive' 'positive' ... 'positive' 'positive' 'positive']\n"]}],"source":["#Predicting the model for bag of words\n","svm_bow_predict=svm.predict(cv_test_reviews)\n","print(svm_bow_predict)\n","#Predicting the model for tfidf features\n","svm_tfidf_predict=svm.predict(tv_test_reviews)\n","print(svm_tfidf_predict)"]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["svm_bow_score : 0.5829\n","svm_tfidf_score : 0.5112\n"]}],"source":["#Accuracy score for bag of words\n","svm_bow_score=accuracy_score(test_sentiments,svm_bow_predict)\n","print(\"svm_bow_score :\",svm_bow_score)\n","#Accuracy score for tfidf features\n","svm_tfidf_score=accuracy_score(test_sentiments,svm_tfidf_predict)\n","print(\"svm_tfidf_score :\",svm_tfidf_score)"]},{"cell_type":"code","execution_count":166,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","    Positive       0.94      0.18      0.30      4993\n","    Negative       0.55      0.99      0.70      5007\n","\n","    accuracy                           0.58     10000\n","   macro avg       0.74      0.58      0.50     10000\n","weighted avg       0.74      0.58      0.50     10000\n","\n","              precision    recall  f1-score   support\n","\n","    Positive       1.00      0.02      0.04      4993\n","    Negative       0.51      1.00      0.67      5007\n","\n","    accuracy                           0.51     10000\n","   macro avg       0.75      0.51      0.36     10000\n","weighted avg       0.75      0.51      0.36     10000\n","\n"]}],"source":["#Classification report for bag of words \n","svm_bow_report=classification_report(test_sentiments,svm_bow_predict,target_names=['Positive','Negative'])\n","print(svm_bow_report)\n","#Classification report for tfidf features\n","svm_tfidf_report=classification_report(test_sentiments,svm_tfidf_predict,target_names=['Positive','Negative'])\n","print(svm_tfidf_report)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Since random guess will have 50% accuracy, we can conclude that SDGC (bags of word accuracy: 58%, tf-idf accuracy: 51%) isn't a good model to implement. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## XGboost"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[{"data":{"text/plain":["(0    20007\n"," 1    19993\n"," Name: sentiment, dtype: int64,\n"," 1    5007\n"," 0    4993\n"," Name: sentiment, dtype: int64)"]},"execution_count":167,"metadata":{},"output_type":"execute_result"}],"source":["train_sentiments_label = train_sentiments.apply(lambda x: 1 if x == 'positive' else 0)\n","test_sentiments_label = test_sentiments.apply(lambda x: 1 if x == 'positive' else 0)\n","train_sentiments_label.value_counts(), test_sentiments_label.value_counts()"]},{"cell_type":"code","execution_count":168,"metadata":{},"outputs":[],"source":["import xgboost as xgb"]},{"cell_type":"code","execution_count":193,"metadata":{},"outputs":[],"source":["cv_classifier = xgb.XGBClassifier(max_depth = 7, eta = 0.9, objective= 'binary:hinge', n_estimators = 200, \n","                                  use_label_encoder=False, eval_metric = 'auc')\n","tv_classifier = xgb.XGBClassifier(max_depth = 10, eta = 0.2, objective= 'binary:hinge', n_estimators = 200, \n","                                  use_label_encoder=False, eval_metric = 'auc')\n","\n","cv_bow = cv_classifier.fit(cv_train_reviews, train_sentiments_label)\n","cv_tfidf = tv_classifier.fit(tv_train_reviews, train_sentiments_label)"]},{"cell_type":"code","execution_count":194,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","    Positive       1.00      0.50      0.67      9972\n","    Negative       0.00      0.61      0.01        28\n","\n","    accuracy                           0.50     10000\n","   macro avg       0.50      0.55      0.34     10000\n","weighted avg       1.00      0.50      0.66     10000\n","\n","              precision    recall  f1-score   support\n","\n","    Positive       0.00      0.00      0.00         0\n","    Negative       1.00      0.50      0.67     10000\n","\n","    accuracy                           0.50     10000\n","   macro avg       0.50      0.25      0.33     10000\n","weighted avg       1.00      0.50      0.67     10000\n","\n"]},{"name":"stderr","output_type":"stream","text":["/Users/swimmingcircle/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/Users/swimmingcircle/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/Users/swimmingcircle/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["xgb_bow_pred = cv_classifier.predict(cv_test_reviews)\n","xgb_tfidf_pred = tv_classifier.predict(tv_test_reviews)\n","\n","# evaluate predictions\n","cv_score = classification_report(xgb_bow_pred, test_sentiments_label, target_names=['Positive','Negative'])\n","print(cv_score)\n","\n","tv_score = classification_report(xgb_tfidf_pred, test_sentiments_label, target_names=['Positive','Negative'])\n","print(tv_score)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["XGboost doesn't seem to be good at handling language transformed texts. It is very sensitive to parameter changes. It often predict the all the data into one class or another when changing the parameters. We conclude that it isn't a good model for our sentiment classification. \n"]},{"cell_type":"markdown","metadata":{},"source":["## BERT model"]},{"cell_type":"markdown","metadata":{},"source":["### What's special about BERT? \n","- Context-free models:generate a single word embedding representation for each word in the vocabulary,  such as word2vec or GloVe. For example, the word “bank” would have the same representation in “bank deposit” and in “riverbank”\n","- Contextual models instead generate a representation of each word that is based on the other words in the sentence, such as BERT."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Understand how BERT works\n","1. Token embeddings: A [CLS] token is added to the input word tokens at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n","2. Segment embeddings: A marker indicating Sentence A or Sentence B is added to each token. **This allows the encoder to distinguish between sentences.**\n","3. Positional embeddings: A positional embedding is added to each token to indicate its position in the sentence.\n"]},{"cell_type":"markdown","metadata":{},"source":["1. Masked LM (MLM)\n","The idea here is “simple”: Randomly mask out 15% of the words in the input — replacing them with a [MASK] token.  Loss function considers only the prediction of the masked tokens and ignores the prediction of the non-masked ones.\n","\n","2. Next Sentence Prediction (NSP)\n","In order to understand relationship between two sentences, BERT training process also uses next sentence prediction, BERT separates sentences with a special [SEP] token. During training the model is fed with two input sentences at a time such that:\n","- 50% of the time the second sentence comes after the first one.\n","- 50% of the time it is a a random sentence from the full corpus.\n","\n","Example: predict if the next sentence is random or not \n","\n","![BERTseq](https://towardsml.files.wordpress.com/2019/09/nsp-1.png)\n","\n","\n","Important note: BERT does not try to predict the next word in the sentence!!"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenizer for BERT\n","\n","BERT uses what is called a WordPiece tokenizer. It works by splitting words either into the full forms (e.g., one word becomes one token) or into word pieces — where one word can be broken into multiple tokens.\n","\n","| Word          | Token(s)                           |\n","| ------------- | ---------------------------------- |\n","| surf          | \\['surf'\\]                         |\n","| surfing       | \\['surf', '##ing'\\]                 |\n","| surfboarding  | \\['surf', '##board', '##ing'\\]       |\n","| surfboard     | \\['surf', '##board'\\]               |\n","| snowboard     | \\['snow', '##board'\\]               |\n","| snowboarding  | \\['snow', '##board', '##ing'\\]       |\n","| snow          | \\['snow'\\]                         |\n","| snowing       | \\['snow', '##ing'\\]                 |\n","\n","By splitting words into word pieces, we have already identified that the words \"surfboard\" and \"snowboard\" share meaning through the wordpiece \"##board\" We have done this without even encoding our tokens or processing them in any way through BERT."]},{"cell_type":"markdown","metadata":{},"source":["### BERT model choice\n","BERT model we choose \n","**DistilBERT** vs BERT\n","- DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\n","\n","**BERT-base** vs BERT-large: BERT-based\n","- BERT-Base: 12-layer, 768-hidden-nodes, 12-attention-heads, 110M parameters\n","- BERT-Large: 24-layer, 1024-hidden-nodes, 16-attention-heads, 340M parameters\n","\n","BERT-based-case vs **BERT-base-uncased**:\n","- We don't differentiate between cased and uncased data (english vs English)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### BERT input \n","\n","Input IDs – The input ids are often the only required parameters to be passed to the model as input. Token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n","\n","Attention mask – Attention Mask is used to avoid performing attention on padding token indices. Mask value can be either 0 or 1, 1 for tokens that are NOT MASKED, 0 for MASKED tokens.\n","\n","Token type ids – It is used in use cases like sequence classification or question answering. As these require two different sequences to be encoded in the same input IDs. Special tokens, such as the classifier[CLS] and separator[SEP] tokens are used to separate the sequences.\n","\n","\n","Note: Padding is a special form of masking where the masked steps are at the start or the end of a sequence. Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences\n","\n","### BERT tokens\n","\n","`CLS`: The [CLS] token, short for \"classification,\" is a special token used in BERT to represent the entire input sequence for classification tasks. \n","\n","When training a classification model using BERT, the [CLS] token is added to the beginning of the input sequence, and the final hidden state corresponding to this token is used as the input to a classifier. This allows the model to make a prediction for the entire input sequence.\n","\n","`SEP`: The [SEP] token, short for \"separator,\" is used to separate two different segments of a sentence or document. \n","\n","In BERT, the [SEP] token is used to separate the two segments when performing tasks like question answering or natural language inference, where the model needs to understand the relationship between two different segments of text.\n","\n","`MASK`: [MASK] is used during pre-training to randomly mask some of the input tokens, forcing the model to learn to predict the masked tokens based on the surrounding context.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Understanding the parameters \n","`max_length` is a parameter used to define the maximum length of an input sequence. \n","\n","`pad_to_max_length` is a Boolean parameter used to indicate whether sequences shorter than the max_length should be padded with a special token, usually [PAD], to make them the same length as the longest sequence in the batch. \n","\n","`return_tensors` parameter specifies that we want the encoded data to be returned as TensorFlow tensor\n","\n","`attention_mask`: 1 indicates a value that should be attended to, while 0 indicates a padded value. \n","\n","Example: \n","\n","`sequence_a = \"This is a short sequence.\"`\n","`sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"`\n","`len(encoded_sequence_a), len(encoded_sequence_b)`\n","\n","(8, 19)\n","\n","`padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)`\n","`padded_sequences[\"input_ids\"]`\n","\n","[[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]\n","\n","`padded_sequences[\"attention_mask\"]`\n","\n","[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[],"source":["#test with a smaller dataset \n","imdb_data = imdb_data[:300]"]},{"cell_type":"code","execution_count":172,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T10:11:53.420885Z","iopub.status.busy":"2023-03-01T10:11:53.420495Z","iopub.status.idle":"2023-03-01T10:11:53.461440Z","shell.execute_reply":"2023-03-01T10:11:53.459354Z","shell.execute_reply.started":"2023-03-01T10:11:53.420839Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)"]},"execution_count":172,"metadata":{},"output_type":"execute_result"}],"source":["import transformers\n","from tokenizers import BertWordPieceTokenizer\n","# First load the real tokenizer\n","tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased' , lower = True)\n","# Save the loaded tokenizer locally\n","tokenizer.save_pretrained('.')\n","# Reload it with the huggingface tokenizers library"]},{"cell_type":"markdown","metadata":{},"source":["Resources\n","- [BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.wordpress.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)"]},{"cell_type":"code","execution_count":195,"metadata":{},"outputs":[],"source":["# Encode the training data\n","encoded_train_data = tokenizer(train_reviews.values.tolist(), padding=True, truncation=True, return_tensors='pt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/ipykernel_8024/2238400034.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(encoded_train_data['input_ids'])\n","/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/ipykernel_8024/2238400034.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  attention_masks = torch.tensor(encoded_train_data['attention_mask'])\n","100%|██████████| 38/38 [05:53<00:00,  9.30s/it]\n"]}],"source":["from torch.utils.data import TensorDataset, DataLoader\n","import torch\n","from tqdm import tqdm\n","from transformers import BertModel\n","\n","# Load the pre-trained BERT model\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# Convert data to PyTorch tensors\n","input_ids = torch.tensor(encoded_train_data['input_ids'])\n","attention_masks = torch.tensor(encoded_train_data['attention_mask'])\n","train_sentiments = train_sentiments.apply(lambda x: 1 if x == 'positive' else 0)\n","labels = torch.tensor(train_sentiments.values.tolist())\n","\n","# Create a TensorDataset\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# Define batch size\n","batch_size = 8\n","\n","# Create a DataLoader\n","dataloader = DataLoader(\n","    dataset,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","# Iterate over batches\n","for batch in tqdm(dataloader):\n","    batch_input_ids = batch[0]\n","    batch_attention_masks = batch[1]\n","    batch_labels = batch[2]\n","    outputs = bert_model(batch_input_ids, attention_mask=batch_attention_masks)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install xelatex"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"c9b5a0d0397a965b8a61bea250a52cf791a7dd32e6dbdab8d82b426f9cd3b168"}}},"nbformat":4,"nbformat_minor":4}
